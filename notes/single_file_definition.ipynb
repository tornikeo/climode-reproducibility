{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Namespace(solver='euler', atol=0.005, rtol=0.005, step_size=None, niters=300, scale=0, batch_size=6, spectral=0, lr=0.0005, weight_decay=1e-05)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import datasets\n",
    "import warnings\n",
    "from tqdm.cli import tqdm\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as Fin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from torchdiffeq import odeint as odeint\n",
    "import matplotlib\n",
    "import argparse\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "cwd = os.getcwd()\n",
    "# data_path = {'z500':str(cwd) + '/era5_data/geopotential_500/*.nc','t850':str(cwd) + '/era5_data/temperature_850/*.nc'}\n",
    "SOLVERS = [\n",
    "    \"dopri8\",\n",
    "    \"dopri5\",\n",
    "    \"bdf\",\n",
    "    \"rk4\",\n",
    "    \"midpoint\",\n",
    "    \"adams\",\n",
    "    \"explicit_adams\",\n",
    "    \"fixed_adams\",\n",
    "    \"adaptive_heun\",\n",
    "    \"euler\",\n",
    "]\n",
    "parser = argparse.ArgumentParser(\"ClimODE\")\n",
    "\n",
    "parser.add_argument(\"--solver\", type=str, default=\"euler\", choices=SOLVERS)\n",
    "parser.add_argument(\"--atol\", type=float, default=5e-3)\n",
    "parser.add_argument(\"--rtol\", type=float, default=5e-3)\n",
    "parser.add_argument(\n",
    "    \"--step_size\", type=float, default=None, help=\"Optional fixed step size.\"\n",
    ")\n",
    "parser.add_argument(\"--niters\", type=int, default=300)\n",
    "parser.add_argument(\"--scale\", type=int, default=0)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=6)\n",
    "parser.add_argument(\"--spectral\", type=int, default=0, choices=[0, 1])\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0005)\n",
    "parser.add_argument(\"--weight_decay\", type=float, default=1e-5)\n",
    "\n",
    "\n",
    "args = parser.parse_args(\"--scale 0 --batch_size 6 --spectral 0 --solver euler\".split())\n",
    "assert torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\")\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_time_scale = slice(\"2006\", \"2016\")\n",
    "val_time_scale = slice(\"2016\", \"2016\")\n",
    "test_time_scale = slice(\"2017\", \"2018\")\n",
    "paths_to_data = [\n",
    "    \"era5_data/geopotential_500/*.nc\",\n",
    "    \"era5_data/temperature_850/*.nc\",\n",
    "    \"era5_data/2m_temperature/*.nc\",\n",
    "    \"era5_data/10m_u_component_of_wind/*.nc\",\n",
    "    \"era5_data/10m_v_component_of_wind/*.nc\",\n",
    "]\n",
    "const_info_path = [\"era5_data/constants/constants/constants_5.625deg.nc\"]\n",
    "levels = [\"z\", \"t\", \"t2m\", \"u10\", \"v10\"]\n",
    "\n",
    "assert len(paths_to_data) == len(\n",
    "    levels\n",
    "), \"Paths to different type of data must be same as number of types of observations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading data:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading data: 100%|██████████| 5/5 [00:21<00:00,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train, val, test data shapes:\n",
      "torch.Size([1460, 10, 5, 32, 64]) torch.Size([1460, 2, 5, 32, 64]) torch.Size([1460, 1, 5, 32, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "\n",
    "\n",
    "def get_batched(train_times, data_train_final, lev):\n",
    "    for idx, year in enumerate(train_times):\n",
    "        data_per_year = data_train_final.sel(time=slice(str(year), str(year))).load()\n",
    "        data_values = data_per_year[lev].values\n",
    "        if idx == 0:\n",
    "            train_data = torch.from_numpy(data_values).reshape(\n",
    "                -1, 1, 1, data_values.shape[-2], data_values.shape[-1]\n",
    "            )\n",
    "            if year % 4 == 0:\n",
    "                train_data = torch.cat(\n",
    "                    (train_data[:236], train_data[240:])\n",
    "                )  # skipping 29 feb in leap year\n",
    "        else:\n",
    "            mid_data = torch.from_numpy(data_values).reshape(\n",
    "                -1, 1, 1, data_values.shape[-2], data_values.shape[-1]\n",
    "            )\n",
    "            if year % 4 == 0:\n",
    "                mid_data = torch.cat(\n",
    "                    (mid_data[:236], mid_data[240:])\n",
    "                )  # skipping 29 feb in leap year\n",
    "            train_data = torch.cat([train_data, mid_data], dim=1)\n",
    "\n",
    "    return train_data\n",
    "\n",
    "\n",
    "def get_train_test_data_without_scales_batched(\n",
    "    data_path, train_time_scale, val_time_scale, test_time_scale, lev, spectral\n",
    "):\n",
    "    data = xr.open_mfdataset(data_path, combine=\"by_coords\")\n",
    "    # data = data.isel(lat=slice(None, None, -1))\n",
    "    if lev in [\"v\", \"u\", \"r\", \"q\", \"tisr\"]:\n",
    "        data = data.sel(level=500)\n",
    "    data = data.resample(time=\"6h\").nearest(\n",
    "        tolerance=\"1h\"\n",
    "    )  # Setting data to be 6-hour cycles\n",
    "    data_train = data.sel(time=train_time_scale).load()\n",
    "    data_val = data.sel(time=val_time_scale).load()\n",
    "    data_test = data.sel(time=test_time_scale).load()\n",
    "    data_global = data.sel(time=slice(\"2006\", \"2018\")).load()\n",
    "\n",
    "    max_val = data_global.max()[lev].values.tolist()\n",
    "    min_val = data_global.min()[lev].values.tolist()\n",
    "\n",
    "    data_train_final = (data_train - min_val) / (max_val - min_val)\n",
    "    data_val_final = (data_val - min_val) / (max_val - min_val)\n",
    "    data_test_final = (data_test - min_val) / (max_val - min_val)\n",
    "\n",
    "    time_vals = data_test_final.time.values\n",
    "    train_times = [i for i in range(2006, 2016)]\n",
    "    test_times = [2017, 2018]\n",
    "    val_times = [2016]\n",
    "\n",
    "    train_data = get_batched(train_times, data_train_final, lev)\n",
    "    test_data = get_batched(test_times, data_test_final, lev)\n",
    "    val_data = get_batched(val_times, data_val_final, lev)\n",
    "\n",
    "    t = [i for i in range(365 * 4)]\n",
    "    time_steps = torch.tensor(t).view(-1, 1)\n",
    "    return (\n",
    "        train_data,\n",
    "        val_data,\n",
    "        test_data,\n",
    "        time_steps,\n",
    "        data.lat.values,\n",
    "        data.lon.values,\n",
    "        max_val,\n",
    "        min_val,\n",
    "        time_vals,\n",
    "    )\n",
    "\n",
    "\n",
    "Final_train_data = 0\n",
    "Final_val_data = 0\n",
    "Final_test_data = 0\n",
    "max_lev = []\n",
    "min_lev = []\n",
    "\n",
    "for idx, data in enumerate(tqdm(paths_to_data, desc=\"reading data\")):\n",
    "    Train_data, Val_data, Test_data, time_steps, lat, lon, mean, std, time_stamp = (\n",
    "        get_train_test_data_without_scales_batched(\n",
    "            data,\n",
    "            train_time_scale,\n",
    "            val_time_scale,\n",
    "            test_time_scale,\n",
    "            levels[idx],\n",
    "            args.spectral,\n",
    "        )\n",
    "    )\n",
    "    max_lev.append(mean)\n",
    "    min_lev.append(std)\n",
    "    if idx == 0:\n",
    "        Final_train_data = Train_data\n",
    "        Final_val_data = Val_data\n",
    "        Final_test_data = Test_data\n",
    "    else:\n",
    "        Final_train_data = torch.cat([Final_train_data, Train_data], dim=2)\n",
    "        Final_val_data = torch.cat([Final_val_data, Val_data], dim=2)\n",
    "        Final_test_data = torch.cat([Final_test_data, Test_data], dim=2)\n",
    "\n",
    "print(\"train, val, test data shapes:\")\n",
    "print(Final_train_data.shape, Final_test_data.shape, Final_val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_velocity(types):\n",
    "    cwd = os.getcwd()\n",
    "    vel = []\n",
    "    for file in types:\n",
    "        vel.append(np.load(str(cwd) + \"/\" + file + \"_vel.npy\"))\n",
    "\n",
    "    return (torch.from_numpy(v) for v in vel)\n",
    "\n",
    "kernel = torch.from_numpy(np.load(str(cwd) + \"/kernel.npy\"))\n",
    "vel_train, vel_val = load_velocity([\"train_10year_2day_mm\", \"val_10year_2day_mm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_constant_info(path):\n",
    "    data = xr.open_mfdataset(path, combine=\"by_coords\")\n",
    "    for idx, var in enumerate([\"orography\", \"lsm\"]):\n",
    "        var_value = torch.from_numpy(data[var].values).view(1, 1, 32, 64)\n",
    "        if idx == 0:\n",
    "            final_var = var_value\n",
    "        else:\n",
    "            final_var = torch.cat([final_var, var_value], dim=1)\n",
    "\n",
    "    return (\n",
    "        final_var,\n",
    "        torch.from_numpy(data[\"lat2d\"].values),\n",
    "        torch.from_numpy(data[\"lon2d\"].values),\n",
    "    )\n",
    "\n",
    "\n",
    "const_channels_info, lat_map, lon_map = add_constant_info(const_info_path)\n",
    "H, W = Train_data.shape[3], Train_data.shape[4]\n",
    "Train_loader = DataLoader(\n",
    "    Final_train_data[2:], batch_size=args.batch_size, shuffle=False, pin_memory=False\n",
    ")\n",
    "Val_loader = DataLoader(\n",
    "    Final_val_data[2:], batch_size=args.batch_size, shuffle=False, pin_memory=False\n",
    ")\n",
    "Test_loader = DataLoader(\n",
    "    Final_test_data[2:], batch_size=args.batch_size, shuffle=False, pin_memory=False\n",
    ")\n",
    "time_loader = DataLoader(\n",
    "    time_steps[2:], batch_size=args.batch_size, shuffle=False, pin_memory=False\n",
    ")\n",
    "time_idx_steps = torch.tensor([i for i in range(365 * 4)]).view(-1, 1)\n",
    "time_idx = DataLoader(\n",
    "    time_idx_steps[2:], batch_size=args.batch_size, shuffle=False, pin_memory=False\n",
    ")\n",
    "\n",
    "# Model declaration\n",
    "num_years = len(range(2006, 2016))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        activation: str = \"gelu\",\n",
    "        norm: bool = False,\n",
    "        n_groups: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.activation = nn.LeakyReLU(0.3)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, out_channels, kernel_size=(3, 3), padding=0\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.drop = nn.Dropout(p=0.1)\n",
    "        # If the number of input channels is not equal to the number of output channels we have to\n",
    "        # project the shortcut connection\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        if norm:\n",
    "            self.norm1 = nn.GroupNorm(n_groups, in_channels)\n",
    "            self.norm2 = nn.GroupNorm(n_groups, out_channels)\n",
    "        else:\n",
    "            self.norm1 = nn.Identity()\n",
    "            self.norm2 = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # First convolution layer\n",
    "        x_mod = F.pad(F.pad(x, (0, 0, 1, 1), \"reflect\"), (1, 1, 0, 0), \"circular\")\n",
    "        h = self.activation(self.bn1(self.conv1(self.norm1(x_mod))))\n",
    "        # Second convolution layer\n",
    "        h = F.pad(F.pad(h, (0, 0, 1, 1), \"reflect\"), (1, 1, 0, 0), \"circular\")\n",
    "        h = self.activation(self.bn2(self.conv2(self.norm2(h))))\n",
    "        h = self.drop(h)\n",
    "        # Add the shortcut connection and return\n",
    "        return h + self.shortcut(x)\n",
    "    \n",
    "class Climate_ResNet_2D(nn.Module):\n",
    "\n",
    "    def __init__(self, num_channels, layers, hidden_size):\n",
    "        super().__init__()\n",
    "        layers_cnn = []\n",
    "        activation_fns = []\n",
    "        self.block = ResidualBlock\n",
    "        self.inplanes = num_channels\n",
    "\n",
    "        for idx in range(len(layers)):\n",
    "            if idx == 0:\n",
    "                layers_cnn.append(\n",
    "                    self.make_layer(\n",
    "                        self.block, num_channels, hidden_size[idx], layers[idx]\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                layers_cnn.append(\n",
    "                    self.make_layer(\n",
    "                        self.block, hidden_size[idx - 1], hidden_size[idx], layers[idx]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        self.layer_cnn = nn.ModuleList(layers_cnn)\n",
    "        self.activation_cnn = nn.ModuleList(activation_fns)\n",
    "\n",
    "    def make_layer(self, block, in_channels, out_channels, reps):\n",
    "        layers = []\n",
    "        layers.append(block(in_channels, out_channels))\n",
    "        self.inplanes = out_channels\n",
    "        for i in range(1, reps):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, data):\n",
    "        dx_final = data.float()\n",
    "        for l, layer in enumerate(self.layer_cnn):\n",
    "            dx_final = layer(dx_final)\n",
    "\n",
    "        return dx_final\n",
    "\n",
    "class boundarypad(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.pad(F.pad(input, (0, 0, 1, 1), \"reflect\"), (1, 1, 0, 0), \"circular\")\n",
    "\n",
    "class Self_attn_conv_reg(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Self_attn_conv_reg, self).__init__()\n",
    "        self.query = self._conv(in_channels, in_channels // 8, stride=1)\n",
    "        self.key = self.key_conv(in_channels, in_channels // 8, stride=2)\n",
    "        self.value = self.key_conv(in_channels, out_channels, stride=2)\n",
    "        self.post_map = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                out_channels, out_channels, kernel_size=(1, 1), stride=1, padding=0\n",
    "            )\n",
    "        )\n",
    "        self.out_ch = out_channels\n",
    "\n",
    "    def _conv(self, n_in, n_out, stride):\n",
    "        return nn.Sequential(\n",
    "            boundarypad(),\n",
    "            nn.Conv2d(n_in, n_in // 2, kernel_size=(3, 3), stride=stride, padding=0),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            boundarypad(),\n",
    "            nn.Conv2d(n_in // 2, n_out, kernel_size=(3, 3), stride=stride, padding=0),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            boundarypad(),\n",
    "            nn.Conv2d(n_out, n_out, kernel_size=(3, 3), stride=stride, padding=0),\n",
    "        )\n",
    "\n",
    "    def key_conv(self, n_in, n_out, stride):\n",
    "        return nn.Sequential(\n",
    "            boundarypad(),\n",
    "            nn.Conv2d(n_in, n_in // 2, kernel_size=(3, 3), stride=stride, padding=0),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            boundarypad(),\n",
    "            nn.Conv2d(n_in // 2, n_out, kernel_size=(3, 3), stride=stride, padding=0),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            boundarypad(),\n",
    "            nn.Conv2d(n_out, n_out, kernel_size=(3, 3), stride=1, padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.size()\n",
    "        x = x.float()\n",
    "        q, k, v = (\n",
    "            self.query(x).flatten(-2, -1),\n",
    "            self.key(x).flatten(-2, -1),\n",
    "            self.value(x).flatten(-2, -1),\n",
    "        )\n",
    "        beta = F.softmax(torch.bmm(q.transpose(1, 2), k), dim=1)\n",
    "        o = torch.bmm(v, beta.transpose(1, 2))\n",
    "        o = self.post_map(o.view(-1, self.out_ch, size[-2], size[-1]).contiguous())\n",
    "        return o\n",
    "\n",
    "class Self_attn_conv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Self_attn_conv, self).__init__()\n",
    "        self.query = self._conv(in_channels, in_channels // 8, stride=1)\n",
    "        self.key = self.key_conv(in_channels, in_channels // 8, stride=2)\n",
    "        self.value = self.key_conv(in_channels, out_channels, stride=2)\n",
    "        self.post_map = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                out_channels, out_channels, kernel_size=(1, 1), stride=1, padding=0\n",
    "            )\n",
    "        )\n",
    "        self.out_ch = out_channels\n",
    "\n",
    "    def _conv(self, n_in, n_out, stride):\n",
    "        return nn.Sequential(\n",
    "            boundarypad(),\n",
    "            nn.Conv2d(n_in, n_in // 2, kernel_size=(3, 3), stride=stride, padding=0),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            boundarypad(),\n",
    "            nn.Conv2d(n_in // 2, n_out, kernel_size=(3, 3), stride=stride, padding=0),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            boundarypad(),\n",
    "            nn.Conv2d(n_out, n_out, kernel_size=(3, 3), stride=stride, padding=0),\n",
    "        )\n",
    "\n",
    "    def key_conv(self, n_in, n_out, stride):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(n_in, n_in // 2, kernel_size=(3, 3), stride=stride, padding=0),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.Conv2d(n_in // 2, n_out, kernel_size=(3, 3), stride=stride, padding=0),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.Conv2d(n_out, n_out, kernel_size=(3, 3), stride=1, padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.size()\n",
    "        x = x.float()\n",
    "        q, k, v = (\n",
    "            self.query(x).flatten(-2, -1),\n",
    "            self.key(x).flatten(-2, -1),\n",
    "            self.value(x).flatten(-2, -1),\n",
    "        )\n",
    "        beta = F.softmax(torch.bmm(q.transpose(1, 2), k), dim=1)\n",
    "        o = torch.bmm(v, beta.transpose(1, 2))\n",
    "        o = self.post_map(o.view(-1, self.out_ch, size[-2], size[-1]).contiguous())\n",
    "        return o\n",
    "\n",
    "\n",
    "class Climate_encoder_free_uncertain(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, num_channels, const_channels, out_types, method, use_att, use_err, use_pos\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = [5, 3, 2]\n",
    "        self.hidden = [128, 64, 2 * out_types]\n",
    "        input_channels = 30 + out_types * int(use_pos) + 34 * (1 - int(use_pos))\n",
    "        self.vel_f = Climate_ResNet_2D(input_channels, self.layers, self.hidden)\n",
    "\n",
    "        if use_att:\n",
    "            self.vel_att = Self_attn_conv(input_channels, 10)\n",
    "            self.gamma = nn.Parameter(torch.tensor([0.1]))\n",
    "\n",
    "        self.scales = num_channels\n",
    "        self.const_channel = const_channels\n",
    "\n",
    "        self.out_ch = out_types\n",
    "        self.past_samples = 0\n",
    "        self.const_info = 0\n",
    "        self.lat_map = 0\n",
    "        self.lon_map = 0\n",
    "        self.elev = 0\n",
    "        self.pos_emb = 0\n",
    "        self.elev_info_grad_x = 0\n",
    "        self.elev_info_grad_y = 0\n",
    "        self.method = method\n",
    "        err_in = 9 + out_types * int(use_pos) + 34 * (1 - int(use_pos))\n",
    "        if use_err:\n",
    "            self.noise_net = Climate_ResNet_2D(\n",
    "                err_in, [3, 2, 2], [128, 64, 2 * out_types]\n",
    "            )\n",
    "        if use_pos:\n",
    "            self.pos_enc = Climate_ResNet_2D(4, [2, 1, 1], [32, 16, out_types])\n",
    "        self.att = use_att\n",
    "        self.err = use_err\n",
    "        self.pos = use_pos\n",
    "        self.pos_feat = 0\n",
    "        self.lsm = 0\n",
    "        self.oro = 0\n",
    "\n",
    "    def update_param(self, params):\n",
    "        self.past_samples = params[0]\n",
    "        self.const_info = params[1]\n",
    "        self.lat_map = params[2]\n",
    "        self.lon_map = params[3]\n",
    "\n",
    "    def pde(self, t, vs):\n",
    "\n",
    "        ds = (\n",
    "            vs[:, -self.out_ch :, :, :]\n",
    "            .view(-1, self.out_ch, vs.shape[2], vs.shape[3])\n",
    "            .float()\n",
    "        )\n",
    "        v = (\n",
    "            vs[:, : 2 * self.out_ch, :, :]\n",
    "            .view(-1, 2 * self.out_ch, vs.shape[2], vs.shape[3])\n",
    "            .float()\n",
    "        )\n",
    "        t_emb = (\n",
    "            ((t * 100) % 24)\n",
    "            .view(1, 1, 1, 1)\n",
    "            .expand(ds.shape[0], 1, ds.shape[2], ds.shape[3])\n",
    "        )\n",
    "        sin_t_emb = torch.sin(torch.pi * t_emb / 12 - torch.pi / 2)\n",
    "        cos_t_emb = torch.cos(torch.pi * t_emb / 12 - torch.pi / 2)\n",
    "\n",
    "        sin_seas_emb = torch.sin(torch.pi * t_emb / (12 * 365) - torch.pi / 2)\n",
    "        cos_seas_emb = torch.cos(torch.pi * t_emb / (12 * 365) - torch.pi / 2)\n",
    "\n",
    "        day_emb = torch.cat([sin_t_emb, cos_t_emb], dim=1)\n",
    "        seas_emb = torch.cat([sin_seas_emb, cos_seas_emb], dim=1)\n",
    "\n",
    "        ds_grad_x = torch.gradient(ds, dim=3)[0]\n",
    "        ds_grad_y = torch.gradient(ds, dim=2)[0]\n",
    "        nabla_u = torch.cat([ds_grad_x, ds_grad_y], dim=1)\n",
    "\n",
    "        if self.pos:\n",
    "            comb_rep = torch.cat(\n",
    "                [t_emb / 24, day_emb, seas_emb, nabla_u, v, ds, self.pos_feat], dim=1\n",
    "            )\n",
    "        else:\n",
    "            cos_lat_map, sin_lat_map = torch.cos(self.new_lat_map), torch.sin(\n",
    "                self.new_lat_map\n",
    "            )\n",
    "            cos_lon_map, sin_lon_map = torch.cos(self.new_lon_map), torch.sin(\n",
    "                self.new_lon_map\n",
    "            )\n",
    "            t_cyc_emb = torch.cat([day_emb, seas_emb], dim=1)\n",
    "            pos_feats = torch.cat(\n",
    "                [\n",
    "                    cos_lat_map,\n",
    "                    cos_lon_map,\n",
    "                    sin_lat_map,\n",
    "                    sin_lon_map,\n",
    "                    sin_lat_map * cos_lon_map,\n",
    "                    sin_lat_map * sin_lon_map,\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "            pos_time_ft = self.get_time_pos_embedding(t_cyc_emb, pos_feats)\n",
    "            comb_rep = torch.cat(\n",
    "                [\n",
    "                    t_emb / 24,\n",
    "                    day_emb,\n",
    "                    seas_emb,\n",
    "                    nabla_u,\n",
    "                    v,\n",
    "                    ds,\n",
    "                    self.new_lat_map,\n",
    "                    self.new_lon_map,\n",
    "                    self.lsm,\n",
    "                    self.oro,\n",
    "                    pos_feats,\n",
    "                    pos_time_ft,\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        if self.att:\n",
    "            dv = self.vel_f(comb_rep) + self.gamma * self.vel_att(comb_rep)\n",
    "        else:\n",
    "            dv = self.vel_f(comb_rep)\n",
    "        v_x = (\n",
    "            v[:, : self.out_ch, :, :]\n",
    "            .view(-1, self.out_ch, vs.shape[2], vs.shape[3])\n",
    "            .float()\n",
    "        )\n",
    "        v_y = (\n",
    "            v[:, -self.out_ch :, :, :]\n",
    "            .view(-1, self.out_ch, vs.shape[2], vs.shape[3])\n",
    "            .float()\n",
    "        )\n",
    "\n",
    "        adv1 = v_x * ds_grad_x + v_y * ds_grad_y\n",
    "        adv2 = ds * (torch.gradient(v_x, dim=3)[0] + torch.gradient(v_y, dim=2)[0])\n",
    "\n",
    "        ds = adv1 + adv2\n",
    "\n",
    "        dvs = torch.cat([dv, ds], 1)\n",
    "        return dvs\n",
    "\n",
    "    def get_time_pos_embedding(self, time_feats, pos_feats):\n",
    "        for idx in range(time_feats.shape[1]):\n",
    "            tf = time_feats[:, idx].unsqueeze(dim=1) * pos_feats\n",
    "            if idx == 0:\n",
    "                final_out = tf\n",
    "            else:\n",
    "                final_out = torch.cat([final_out, tf], dim=1)\n",
    "\n",
    "        return final_out\n",
    "\n",
    "    def noise_net_contrib(self, t, pos_enc, s_final, noise_net, H, W):\n",
    "\n",
    "        t_emb = (t % 24).view(-1, 1, 1, 1, 1)\n",
    "        sin_t_emb = torch.sin(torch.pi * t_emb / 12 - torch.pi / 2).expand(\n",
    "            len(s_final), s_final.shape[1], 1, H, W\n",
    "        )\n",
    "        cos_t_emb = torch.cos(torch.pi * t_emb / 12 - torch.pi / 2).expand(\n",
    "            len(s_final), s_final.shape[1], 1, H, W\n",
    "        )\n",
    "\n",
    "        sin_seas_emb = torch.sin(torch.pi * t_emb / (12 * 365) - torch.pi / 2).expand(\n",
    "            len(s_final), s_final.shape[1], 1, H, W\n",
    "        )\n",
    "        cos_seas_emb = torch.cos(torch.pi * t_emb / (12 * 365) - torch.pi / 2).expand(\n",
    "            len(s_final), s_final.shape[1], 1, H, W\n",
    "        )\n",
    "\n",
    "        pos_enc = pos_enc.expand(len(s_final), s_final.shape[1], -1, H, W).flatten(\n",
    "            start_dim=0, end_dim=1\n",
    "        )\n",
    "        t_cyc_emb = torch.cat(\n",
    "            [sin_t_emb, cos_t_emb, sin_seas_emb, cos_seas_emb], dim=2\n",
    "        ).flatten(start_dim=0, end_dim=1)\n",
    "\n",
    "        pos_time_ft = self.get_time_pos_embedding(t_cyc_emb, pos_enc[:, 2:-2])\n",
    "\n",
    "        comb_rep = torch.cat(\n",
    "            [t_cyc_emb, s_final.flatten(start_dim=0, end_dim=1), pos_enc, pos_time_ft],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        final_out = noise_net(comb_rep).view(len(t), -1, 2 * self.out_ch, H, W)\n",
    "\n",
    "        mean = s_final + final_out[:, :, : self.out_ch]\n",
    "        std = nn.Softplus()(final_out[:, :, self.out_ch :])\n",
    "\n",
    "        return mean, std\n",
    "\n",
    "    def forward(self, T, data, atol=0.1, rtol=0.1):\n",
    "        H, W = self.past_samples.shape[2], self.past_samples.shape[3]\n",
    "        final_data = torch.cat(\n",
    "            [self.past_samples, data.float().view(-1, self.out_ch, H, W)], 1\n",
    "        )\n",
    "        init_time = T[0].item() * 6\n",
    "        final_time = T[-1].item() * 6\n",
    "        steps_val = final_time - init_time\n",
    "\n",
    "        # breakpoint()\n",
    "\n",
    "        if self.pos:\n",
    "            lat_map = self.lat_map.unsqueeze(dim=0) * torch.pi / 180\n",
    "            lon_map = self.lon_map.unsqueeze(dim=0) * torch.pi / 180\n",
    "            pos_rep = torch.cat(\n",
    "                [lat_map.unsqueeze(dim=0), lon_map.unsqueeze(dim=0), self.const_info],\n",
    "                dim=1,\n",
    "            )\n",
    "            self.pos_feat = self.pos_enc(pos_rep).expand(\n",
    "                data.shape[0], -1, data.shape[3], data.shape[4]\n",
    "            )\n",
    "            final_pos_enc = self.pos_feat\n",
    "\n",
    "        else:\n",
    "            self.oro, self.lsm = self.const_info[0, 0], self.const_info[0, 1]\n",
    "            self.lsm = self.lsm.unsqueeze(dim=0).expand(\n",
    "                data.shape[0], -1, data.shape[3], data.shape[4]\n",
    "            )\n",
    "            self.oro = (\n",
    "                F.normalize(self.const_info[0, 0])\n",
    "                .unsqueeze(dim=0)\n",
    "                .expand(data.shape[0], -1, data.shape[3], data.shape[4])\n",
    "            )\n",
    "            self.new_lat_map = (\n",
    "                self.lat_map.expand(data.shape[0], 1, data.shape[3], data.shape[4])\n",
    "                * torch.pi\n",
    "                / 180\n",
    "            )  # Converting to radians\n",
    "            self.new_lon_map = (\n",
    "                self.lon_map.expand(data.shape[0], 1, data.shape[3], data.shape[4])\n",
    "                * torch.pi\n",
    "                / 180\n",
    "            )\n",
    "            cos_lat_map, sin_lat_map = torch.cos(self.new_lat_map), torch.sin(\n",
    "                self.new_lat_map\n",
    "            )\n",
    "            cos_lon_map, sin_lon_map = torch.cos(self.new_lon_map), torch.sin(\n",
    "                self.new_lon_map\n",
    "            )\n",
    "            pos_feats = torch.cat(\n",
    "                [\n",
    "                    cos_lat_map,\n",
    "                    cos_lon_map,\n",
    "                    sin_lat_map,\n",
    "                    sin_lon_map,\n",
    "                    sin_lat_map * cos_lon_map,\n",
    "                    sin_lat_map * sin_lon_map,\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "            final_pos_enc = torch.cat(\n",
    "                [self.new_lat_map, self.new_lon_map, pos_feats, self.lsm, self.oro],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        new_time_steps = torch.linspace(\n",
    "            init_time, final_time, steps=int(steps_val) + 1\n",
    "        ).to(data.device)\n",
    "        t = 0.01 * new_time_steps.float().to(data.device).flatten().float()\n",
    "        pde_rhs = lambda t, vs: self.pde(t, vs)  # make the ODE forward function\n",
    "        final_result = odeint(\n",
    "            pde_rhs, final_data, t, method=self.method, atol=atol, rtol=rtol\n",
    "        )\n",
    "        # breakpoint()\n",
    "        s_final = final_result[:, :, -self.out_ch :, :, :].view(\n",
    "            len(t), -1, self.out_ch, H, W\n",
    "        )\n",
    "\n",
    "        if self.err:\n",
    "            mean, std = self.noise_net_contrib(\n",
    "                T, final_pos_enc, s_final[0 : len(s_final) : 6], self.noise_net, H, W\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            s_final = s_final[0 : len(s_final) : 6]\n",
    "\n",
    "        return mean, std, s_final[0 : len(s_final) : 6]\n",
    "\n",
    "model = Climate_encoder_free_uncertain(\n",
    "    len(paths_to_data),\n",
    "    2,\n",
    "    out_types=len(paths_to_data),\n",
    "    method=args.solver,\n",
    "    use_att=True,\n",
    "    use_err=True,\n",
    "    use_pos=False,\n",
    ").to(device)\n",
    "\n",
    "param = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 300)\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "train_best_loss = float(\"inf\")\n",
    "best_epoch = float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Epoch 0 of 300 #####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|\u001b[32m██████████\u001b[0m| 243/243 [04:13<00:00,  1.04s/it, loss=15.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Iter  0  | Total Train Loss  12555.654718399048 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test:  53%|\u001b[34m█████▎    \u001b[0m| 130/243 [00:30<00:26,  4.27it/s, val_lss=6.25]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 95\u001b[0m\n\u001b[1;32m     86\u001b[0m model\u001b[38;5;241m.\u001b[39mupdate_param(\n\u001b[1;32m     87\u001b[0m     [\n\u001b[1;32m     88\u001b[0m         past_sample,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m     ]\n\u001b[1;32m     93\u001b[0m )\n\u001b[1;32m     94\u001b[0m t \u001b[38;5;241m=\u001b[39m time_steps\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m---> 95\u001b[0m mean, std, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m loss \u001b[38;5;241m=\u001b[39m nll(mean, std, batch\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device), lat, var_coeff)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(loss):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 460\u001b[0m, in \u001b[0;36mClimate_encoder_free_uncertain.forward\u001b[0;34m(self, T, data, atol, rtol)\u001b[0m\n\u001b[1;32m    458\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m \u001b[38;5;241m*\u001b[39m new_time_steps\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(data\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    459\u001b[0m pde_rhs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m t, vs: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpde(t, vs)  \u001b[38;5;66;03m# make the ODE forward function\u001b[39;00m\n\u001b[0;32m--> 460\u001b[0m final_result \u001b[38;5;241m=\u001b[39m \u001b[43modeint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpde_rhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrtol\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# breakpoint()\u001b[39;00m\n\u001b[1;32m    464\u001b[0m s_final \u001b[38;5;241m=\u001b[39m final_result[:, :, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_ch :, :, :]\u001b[38;5;241m.\u001b[39mview(\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28mlen\u001b[39m(t), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_ch, H, W\n\u001b[1;32m    466\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchdiffeq/_impl/odeint.py:80\u001b[0m, in \u001b[0;36modeint\u001b[0;34m(func, y0, t, rtol, atol, method, options, event_fn)\u001b[0m\n\u001b[1;32m     77\u001b[0m solver \u001b[38;5;241m=\u001b[39m SOLVERS[method](func\u001b[38;5;241m=\u001b[39mfunc, y0\u001b[38;5;241m=\u001b[39my0, rtol\u001b[38;5;241m=\u001b[39mrtol, atol\u001b[38;5;241m=\u001b[39matol, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m     solution \u001b[38;5;241m=\u001b[39m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintegrate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m     event_t, solution \u001b[38;5;241m=\u001b[39m solver\u001b[38;5;241m.\u001b[39mintegrate_until_event(t[\u001b[38;5;241m0\u001b[39m], event_fn)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchdiffeq/_impl/solvers.py:114\u001b[0m, in \u001b[0;36mFixedGridODESolver.integrate\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m    112\u001b[0m dt \u001b[38;5;241m=\u001b[39m t1 \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\u001b[38;5;241m.\u001b[39mcallback_step(t0, y0, dt)\n\u001b[0;32m--> 114\u001b[0m dy, f0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m y1 \u001b[38;5;241m=\u001b[39m y0 \u001b[38;5;241m+\u001b[39m dy\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m j \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(t) \u001b[38;5;129;01mand\u001b[39;00m t1 \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m t[j]:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchdiffeq/_impl/fixed_grid.py:10\u001b[0m, in \u001b[0;36mEuler._step_func\u001b[0;34m(self, func, t0, dt, t1, y0)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_step_func\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, t0, dt, t1, y0):\n\u001b[0;32m---> 10\u001b[0m     f0 \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperturb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPerturb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNEXT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperturb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mPerturb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNONE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dt \u001b[38;5;241m*\u001b[39m f0, f0\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchdiffeq/_impl/misc.py:197\u001b[0m, in \u001b[0;36m_PerturbFunc.forward\u001b[0;34m(self, t, y, perturb)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Do nothing.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 459\u001b[0m, in \u001b[0;36mClimate_encoder_free_uncertain.forward.<locals>.<lambda>\u001b[0;34m(t, vs)\u001b[0m\n\u001b[1;32m    455\u001b[0m new_time_steps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(\n\u001b[1;32m    456\u001b[0m     init_time, final_time, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(steps_val) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    457\u001b[0m )\u001b[38;5;241m.\u001b[39mto(data\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    458\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m \u001b[38;5;241m*\u001b[39m new_time_steps\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(data\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m--> 459\u001b[0m pde_rhs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m t, vs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpde\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# make the ODE forward function\u001b[39;00m\n\u001b[1;32m    460\u001b[0m final_result \u001b[38;5;241m=\u001b[39m odeint(\n\u001b[1;32m    461\u001b[0m     pde_rhs, final_data, t, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod, atol\u001b[38;5;241m=\u001b[39matol, rtol\u001b[38;5;241m=\u001b[39mrtol\n\u001b[1;32m    462\u001b[0m )\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# breakpoint()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 320\u001b[0m, in \u001b[0;36mClimate_encoder_free_uncertain.pde\u001b[0;34m(self, t, vs)\u001b[0m\n\u001b[1;32m    301\u001b[0m     comb_rep \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m    302\u001b[0m         [\n\u001b[1;32m    303\u001b[0m             t_emb \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m24\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    316\u001b[0m         dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    317\u001b[0m     )\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matt:\n\u001b[0;32m--> 320\u001b[0m     dv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvel_f\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomb_rep\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvel_att(comb_rep)\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    322\u001b[0m     dv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvel_f(comb_rep)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 88\u001b[0m, in \u001b[0;36mClimate_ResNet_2D.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     86\u001b[0m dx_final \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_cnn):\n\u001b[0;32m---> 88\u001b[0m     dx_final \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdx_final\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dx_final\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 42\u001b[0m, in \u001b[0;36mResidualBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# First convolution layer\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     x_mod \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(F\u001b[38;5;241m.\u001b[39mpad(x, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreflect\u001b[39m\u001b[38;5;124m\"\u001b[39m), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcircular\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_mod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Second convolution layer\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(F\u001b[38;5;241m.\u001b[39mpad(h, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreflect\u001b[39m\u001b[38;5;124m\"\u001b[39m), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcircular\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1507\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1504\u001b[0m             tracing_state\u001b[38;5;241m.\u001b[39mpop_scope()\n\u001b[1;32m   1505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 1507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def nll(mean, std, truth, lat, var_coeff):\n",
    "    normal_lkl = torch.distributions.normal.Normal(mean, 1e-3 + std)\n",
    "    lkl = -normal_lkl.log_prob(truth)\n",
    "    loss_val = lkl.mean() + var_coeff * (std**2).sum()\n",
    "    # loss_val = torch.mean(lkl,dim=(0,1,3,4))\n",
    "    return loss_val\n",
    "\n",
    "for epoch in range(args.niters):\n",
    "    print(f\"##### Epoch {epoch} of {args.niters} #####\")\n",
    "    total_train_loss = 0\n",
    "    val_loss = 0\n",
    "    test_loss = 0\n",
    "    # RMSD = []\n",
    "    # breakpoint()\n",
    "    if epoch == 0:\n",
    "        var_coeff = 0.001\n",
    "    else:\n",
    "        var_coeff = 2 * scheduler.get_last_lr()[0]\n",
    "\n",
    "    _total = min(len(time_loader), len(Train_loader))\n",
    "    pbar = tqdm(\n",
    "        enumerate(zip(time_loader, Train_loader)),\n",
    "        total=_total,\n",
    "        colour=\"green\",\n",
    "        desc=\"train\",\n",
    "    )\n",
    "    for entry, (time_steps, batch) in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        data = (\n",
    "            batch[0]\n",
    "            .to(device)\n",
    "            .view(num_years, 1, len(paths_to_data) * (args.scale + 1), H, W)\n",
    "        )\n",
    "        past_sample = (\n",
    "            vel_train[entry]\n",
    "            .view(num_years, 2 * len(paths_to_data) * (args.scale + 1), H, W)\n",
    "            .to(device)\n",
    "        )\n",
    "        model.update_param(\n",
    "            [\n",
    "                past_sample,\n",
    "                const_channels_info.to(device),\n",
    "                lat_map.to(device),\n",
    "                lon_map.to(device),\n",
    "            ]\n",
    "        )\n",
    "        t = time_steps.float().to(device).flatten()\n",
    "        mean, std, _ = model(t, data)\n",
    "        loss = nll(mean, std, batch.float().to(device), lat, var_coeff)\n",
    "        l2_lambda = 0.001\n",
    "        l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "        loss = loss + l2_lambda * l2_norm\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(\"Loss for batch is \",loss.item())\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "        if torch.isnan(loss):\n",
    "            print(\"Quitting due to Nan loss\")\n",
    "            quit()\n",
    "        total_train_loss = total_train_loss + loss.item()\n",
    "\n",
    "    lr_val = scheduler.get_last_lr()[0]\n",
    "    scheduler.step()\n",
    "    print(\"|Iter \", epoch, \" | Total Train Loss \", total_train_loss, \"|\")\n",
    "    optimizer.zero_grad(set_to_none=True)  # Clear memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(\n",
    "            enumerate(zip(time_loader, Val_loader)),\n",
    "            total=min(len(time_loader), len(Val_loader)),\n",
    "            colour=\"blue\",\n",
    "            desc=\"test\",\n",
    "        )\n",
    "        for entry, (time_steps, batch) in pbar:\n",
    "            data = (\n",
    "                batch[0]\n",
    "                .to(device)\n",
    "                .view(1, 1, len(paths_to_data) * (args.scale + 1), H, W)\n",
    "            )\n",
    "            past_sample = (\n",
    "                vel_val[entry]\n",
    "                .view(1, 2 * len(paths_to_data) * (args.scale + 1), H, W)\n",
    "                .to(device)\n",
    "            )\n",
    "            model.update_param(\n",
    "                [\n",
    "                    past_sample,\n",
    "                    const_channels_info.to(device),\n",
    "                    lat_map.to(device),\n",
    "                    lon_map.to(device),\n",
    "                ]\n",
    "            )\n",
    "            t = time_steps.float().to(device).flatten()\n",
    "            mean, std, _ = model(t, data)\n",
    "            loss = nll(mean, std, batch.float().to(device), lat, var_coeff)\n",
    "            if torch.isnan(loss):\n",
    "                print(\"Quitting due to Nan loss\")\n",
    "                quit()\n",
    "            pbar.set_postfix({\"val_lss\": loss.item()})\n",
    "            val_loss = val_loss + loss.item()\n",
    "\n",
    "    print(\"|Iter \", epoch, \" | Total Val Loss \", val_loss, \"|\")\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(\n",
    "            model,\n",
    "            str(cwd)\n",
    "            + \"/Models/\"\n",
    "            + \"ClimODE_global_\"\n",
    "            + args.solver\n",
    "            + \"_\"\n",
    "            + str(args.spectral)\n",
    "            + \"_model_\"\n",
    "            + str(epoch)\n",
    "            + \".pt\",\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
