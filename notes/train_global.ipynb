{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/climode-reproducibility/climode/ClimODE\n"
     ]
    }
   ],
   "source": [
    "# %cd ./climode/ClimODE/\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "import warnings\n",
    "from tqdm.cli import tqdm\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as Fin\n",
    "import timeit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from torchdiffeq import odeint as odeint\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.empty_cache()\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import logging\n",
    "\n",
    "logging.propagate = False\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 42\n"
     ]
    }
   ],
   "source": [
    "# from model_function import *\n",
    "# from model_utils import *\n",
    "from utils import set_seed\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(solver='euler', atol=0.005, rtol=0.005, step_size=None, niters=300, scale=0, batch_size=6, spectral=0, lr=0.0005, weight_decay=1e-05)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "# data_path = {'z500':str(cwd) + '/era5_data/geopotential_500/*.nc','t850':str(cwd) + '/era5_data/temperature_850/*.nc'}\n",
    "SOLVERS = [\n",
    "    \"dopri8\",\n",
    "    \"dopri5\",\n",
    "    \"bdf\",\n",
    "    \"rk4\",\n",
    "    \"midpoint\",\n",
    "    \"adams\",\n",
    "    \"explicit_adams\",\n",
    "    \"fixed_adams\",\n",
    "    \"adaptive_heun\",\n",
    "    \"euler\",\n",
    "]\n",
    "parser = argparse.ArgumentParser(\"ClimODE\")\n",
    "\n",
    "parser.add_argument(\"--solver\", type=str, default=\"euler\", choices=SOLVERS)\n",
    "parser.add_argument(\"--atol\", type=float, default=5e-3)\n",
    "parser.add_argument(\"--rtol\", type=float, default=5e-3)\n",
    "parser.add_argument(\n",
    "    \"--step_size\", type=float, default=None, help=\"Optional fixed step size.\"\n",
    ")\n",
    "parser.add_argument(\"--niters\", type=int, default=300)\n",
    "parser.add_argument(\"--scale\", type=int, default=0)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=6)\n",
    "parser.add_argument(\"--spectral\", type=int, default=0, choices=[0, 1])\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0005)\n",
    "parser.add_argument(\"--weight_decay\", type=float, default=1e-5)\n",
    "\n",
    "\n",
    "args = parser.parse_args(\"--scale 0 --batch_size 6 --spectral 0 --solver euler\".split())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_time_scale = slice(\"2006\", \"2016\")\n",
    "val_time_scale = slice(\"2016\", \"2016\")\n",
    "test_time_scale = slice(\"2017\", \"2018\")\n",
    "cwd = os.getcwd()\n",
    "paths_to_data = [\n",
    "    str(cwd) + \"/era5_data/geopotential_500/*.nc\",\n",
    "    str(cwd) + \"/era5_data/temperature_850/*.nc\",\n",
    "    str(cwd) + \"/era5_data/2m_temperature/*.nc\",\n",
    "    str(cwd) + \"/era5_data/10m_u_component_of_wind/*.nc\",\n",
    "    str(cwd) + \"/era5_data/10m_v_component_of_wind/*.nc\",\n",
    "]\n",
    "const_info_path = [str(cwd) + \"/era5_data/constants/constants/constants_5.625deg.nc\"]\n",
    "levels = [\"z\", \"t\", \"t2m\", \"u10\", \"v10\"]\n",
    "paths_to_data = paths_to_data[0:5]\n",
    "levels = levels[0:5]\n",
    "assert len(paths_to_data) == len(\n",
    "    levels\n",
    "), \"Paths to different type of data must be same as number of types of observations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "\n",
    "def get_batched(train_times, data_train_final, lev):\n",
    "    for idx, year in enumerate(train_times):\n",
    "        data_per_year = data_train_final.sel(time=slice(str(year), str(year))).load()\n",
    "        data_values = data_per_year[lev].values\n",
    "        if idx == 0:\n",
    "            train_data = torch.from_numpy(data_values).reshape(\n",
    "                -1, 1, 1, data_values.shape[-2], data_values.shape[-1]\n",
    "            )\n",
    "            if year % 4 == 0:\n",
    "                train_data = torch.cat(\n",
    "                    (train_data[:236], train_data[240:])\n",
    "                )  # skipping 29 feb in leap year\n",
    "        else:\n",
    "            mid_data = torch.from_numpy(data_values).reshape(\n",
    "                -1, 1, 1, data_values.shape[-2], data_values.shape[-1]\n",
    "            )\n",
    "            if year % 4 == 0:\n",
    "                mid_data = torch.cat(\n",
    "                    (mid_data[:236], mid_data[240:])\n",
    "                )  # skipping 29 feb in leap year\n",
    "            train_data = torch.cat([train_data, mid_data], dim=1)\n",
    "\n",
    "    return train_data\n",
    "\n",
    "\n",
    "def get_train_test_data_without_scales_batched(\n",
    "    data_path, train_time_scale, val_time_scale, test_time_scale, lev, spectral\n",
    "):\n",
    "    data = xr.open_mfdataset(data_path, combine=\"by_coords\")\n",
    "    # data = data.isel(lat=slice(None, None, -1))\n",
    "    if lev in [\"v\", \"u\", \"r\", \"q\", \"tisr\"]:\n",
    "        data = data.sel(level=500)\n",
    "    data = data.resample(time=\"6h\").nearest(\n",
    "        tolerance=\"1h\"\n",
    "    )  # Setting data to be 6-hour cycles\n",
    "    data_train = data.sel(time=train_time_scale).load()\n",
    "    data_val = data.sel(time=val_time_scale).load()\n",
    "    data_test = data.sel(time=test_time_scale).load()\n",
    "    data_global = data.sel(time=slice(\"2006\", \"2018\")).load()\n",
    "\n",
    "    max_val = data_global.max()[lev].values.tolist()\n",
    "    min_val = data_global.min()[lev].values.tolist()\n",
    "\n",
    "    data_train_final = (data_train - min_val) / (max_val - min_val)\n",
    "    data_val_final = (data_val - min_val) / (max_val - min_val)\n",
    "    data_test_final = (data_test - min_val) / (max_val - min_val)\n",
    "\n",
    "    time_vals = data_test_final.time.values\n",
    "    train_times = [i for i in range(2006, 2016)]\n",
    "    test_times = [2017, 2018]\n",
    "    val_times = [2016]\n",
    "\n",
    "    train_data = get_batched(train_times, data_train_final, lev)\n",
    "    test_data = get_batched(test_times, data_test_final, lev)\n",
    "    val_data = get_batched(val_times, data_val_final, lev)\n",
    "\n",
    "    t = [i for i in range(365 * 4)]\n",
    "    time_steps = torch.tensor(t).view(-1, 1)\n",
    "    return (\n",
    "        train_data,\n",
    "        val_data,\n",
    "        test_data,\n",
    "        time_steps,\n",
    "        data.lat.values,\n",
    "        data.lon.values,\n",
    "        max_val,\n",
    "        min_val,\n",
    "        time_vals,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/xarray/groupers.py:403: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  self.index_grouper = pd.Grouper(\n",
      "/opt/conda/lib/python3.10/site-packages/xarray/core/indexes.py:561: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  flat_indexer = index.get_indexer(flat_labels, method=method, tolerance=tolerance)\n",
      "/opt/conda/lib/python3.10/site-packages/xarray/groupers.py:403: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  self.index_grouper = pd.Grouper(\n",
      "/opt/conda/lib/python3.10/site-packages/xarray/core/indexes.py:561: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  flat_indexer = index.get_indexer(flat_labels, method=method, tolerance=tolerance)\n",
      "/opt/conda/lib/python3.10/site-packages/xarray/groupers.py:403: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  self.index_grouper = pd.Grouper(\n",
      "/opt/conda/lib/python3.10/site-packages/xarray/core/indexes.py:561: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  flat_indexer = index.get_indexer(flat_labels, method=method, tolerance=tolerance)\n",
      "/opt/conda/lib/python3.10/site-packages/xarray/groupers.py:403: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  self.index_grouper = pd.Grouper(\n",
      "/opt/conda/lib/python3.10/site-packages/xarray/core/indexes.py:561: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  flat_indexer = index.get_indexer(flat_labels, method=method, tolerance=tolerance)\n",
      "/opt/conda/lib/python3.10/site-packages/xarray/groupers.py:403: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  self.index_grouper = pd.Grouper(\n",
      "/opt/conda/lib/python3.10/site-packages/xarray/core/indexes.py:561: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  flat_indexer = index.get_indexer(flat_labels, method=method, tolerance=tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training data 1460\n",
      "Length of validation data 1460\n",
      "Length of testing data 1460\n"
     ]
    }
   ],
   "source": [
    "Final_train_data = 0\n",
    "Final_val_data = 0\n",
    "Final_test_data = 0\n",
    "max_lev = []\n",
    "min_lev = []\n",
    "\n",
    "for idx, data in enumerate(paths_to_data):\n",
    "    Train_data, Val_data, Test_data, time_steps, lat, lon, mean, std, time_stamp = (\n",
    "        get_train_test_data_without_scales_batched(\n",
    "            data,\n",
    "            train_time_scale,\n",
    "            val_time_scale,\n",
    "            test_time_scale,\n",
    "            levels[idx],\n",
    "            args.spectral,\n",
    "        )\n",
    "    )\n",
    "    max_lev.append(mean)\n",
    "    min_lev.append(std)\n",
    "    if idx == 0:\n",
    "        Final_train_data = Train_data\n",
    "        Final_val_data = Val_data\n",
    "        Final_test_data = Test_data\n",
    "    else:\n",
    "        Final_train_data = torch.cat([Final_train_data, Train_data], dim=2)\n",
    "        Final_val_data = torch.cat([Final_val_data, Val_data], dim=2)\n",
    "        Final_test_data = torch.cat([Final_test_data, Test_data], dim=2)\n",
    "\n",
    "\n",
    "print(\"Length of training data\", len(Final_train_data))\n",
    "print(\"Length of validation data\", len(Final_val_data))\n",
    "print(\"Length of testing data\", len(Final_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from model_function import add_constant_info, Climate_encoder_free_uncertain\n",
    "from utils import count_parameters\n",
    "\n",
    "const_channels_info, lat_map, lon_map = add_constant_info(const_info_path)\n",
    "H, W = Train_data.shape[3], Train_data.shape[4]\n",
    "Train_loader = DataLoader(\n",
    "    Final_train_data[2:], batch_size=args.batch_size, shuffle=False, pin_memory=False\n",
    ")\n",
    "Val_loader = DataLoader(\n",
    "    Final_val_data[2:], batch_size=args.batch_size, shuffle=False, pin_memory=False\n",
    ")\n",
    "Test_loader = DataLoader(\n",
    "    Final_test_data[2:], batch_size=args.batch_size, shuffle=False, pin_memory=False\n",
    ")\n",
    "time_loader = DataLoader(\n",
    "    time_steps[2:], batch_size=args.batch_size, shuffle=False, pin_memory=False\n",
    ")\n",
    "time_idx_steps = torch.tensor([i for i in range(365 * 4)]).view(-1, 1)\n",
    "time_idx = DataLoader(\n",
    "    time_idx_steps[2:], batch_size=args.batch_size, shuffle=False, pin_memory=False\n",
    ")\n",
    "# Model declaration\n",
    "num_years = len(range(2006, 2016))\n",
    "model = Climate_encoder_free_uncertain(\n",
    "    len(paths_to_data),\n",
    "    2,\n",
    "    out_types=len(paths_to_data),\n",
    "    method=args.solver,\n",
    "    use_att=True,\n",
    "    use_err=True,\n",
    "    use_pos=False,\n",
    ").to(device)\n",
    "# model.apply(weights_init_uniform_rule)\n",
    "\n",
    "param = count_parameters(model)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 300)\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "train_best_loss = float(\"inf\")\n",
    "best_epoch = float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting velocity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[32m██████████\u001b[0m| 243/243 [03:27<00:00,  1.17it/s]\n",
      "100%|\u001b[32m██████████\u001b[0m| 243/243 [01:44<00:00,  2.32it/s]\n",
      "100%|\u001b[32m██████████\u001b[0m| 243/243 [01:50<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################ Velocity loaded, Model starts to train #########################\n",
      "####################### Total Parameters 2752099 ################################\n"
     ]
    }
   ],
   "source": [
    "from utils import load_velocity, fit_velocity, get_gauss_kernel\n",
    "from model_function import Optim_velocity\n",
    "\n",
    "if not Path(str(cwd) + \"/kernel.npy\").exists():\n",
    "    get_gauss_kernel((32, 64), lat, lon)\n",
    "else:\n",
    "    print(\n",
    "        \"## DETECTED kernel.npy from previous run, will load from disk instead of regen. \"\n",
    "    )\n",
    "\n",
    "kernel = torch.from_numpy(np.load(str(cwd) + \"/kernel.npy\"))\n",
    "# breakpoint()\n",
    "if not Path(\"test_10year_2day_mm_vel.npy\").exists():\n",
    "    print(\"Fitting velocity...\")\n",
    "    fit_velocity(\n",
    "        time_idx,\n",
    "        time_loader,\n",
    "        Final_train_data,\n",
    "        Train_loader,\n",
    "        torch.device(\"cuda\"),\n",
    "        num_years,\n",
    "        paths_to_data,\n",
    "        args.scale,\n",
    "        H,\n",
    "        W,\n",
    "        types=\"train_10year_2day_mm\",\n",
    "        vel_model=Optim_velocity,\n",
    "        kernel=kernel,\n",
    "        lat=lat,\n",
    "        lon=lon,\n",
    "    )\n",
    "    fit_velocity(\n",
    "        time_idx,\n",
    "        time_loader,\n",
    "        Final_val_data,\n",
    "        Val_loader,\n",
    "        torch.device(\"cuda\"),\n",
    "        1,\n",
    "        paths_to_data,\n",
    "        args.scale,\n",
    "        H,\n",
    "        W,\n",
    "        types=\"val_10year_2day_mm\",\n",
    "        vel_model=Optim_velocity,\n",
    "        kernel=kernel,\n",
    "        lat=lat,\n",
    "        lon=lon,\n",
    "    )\n",
    "    fit_velocity(\n",
    "        time_idx,\n",
    "        time_loader,\n",
    "        Final_test_data,\n",
    "        Test_loader,\n",
    "        torch.device(\"cuda\"),\n",
    "        2,\n",
    "        paths_to_data,\n",
    "        args.scale,\n",
    "        H,\n",
    "        W,\n",
    "        types=\"test_10year_2day_mm\",\n",
    "        vel_model=Optim_velocity,\n",
    "        kernel=kernel,\n",
    "        lat=lat,\n",
    "        lon=lon,\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"## DETECTED test_10year_2day_mm_vel.npy from previous run, will load from disk instead of regen\"\n",
    "    )\n",
    "\n",
    "vel_train, vel_val = load_velocity([\"train_10year_2day_mm\", \"val_10year_2day_mm\"])\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\n",
    "    \"############################ Velocity loaded, Model starts to train #########################\"\n",
    ")\n",
    "# print(model)\n",
    "print(\n",
    "    \"####################### Total Parameters\",\n",
    "    param,\n",
    "    \"################################\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 2.8e+06 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model has {param:.1e} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Epoch 0 of 300 #####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:   0%|\u001b[32m          \u001b[0m| 0/243 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  50%|\u001b[32m█████     \u001b[0m| 122/243 [02:07<02:06,  1.05s/it, loss=20.4] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39mupdate_param([past_sample,const_channels_info\u001b[38;5;241m.\u001b[39mto(device),lat_map\u001b[38;5;241m.\u001b[39mto(device),lon_map\u001b[38;5;241m.\u001b[39mto(device)])\n\u001b[1;32m     24\u001b[0m t \u001b[38;5;241m=\u001b[39m time_steps\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m---> 25\u001b[0m mean,std,_ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m nll(mean,std,batch\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device),lat,var_coeff)\n\u001b[1;32m     27\u001b[0m l2_lambda \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/climode-reproducibility/climode/ClimODE/model_function.py:214\u001b[0m, in \u001b[0;36mClimate_encoder_free_uncertain.forward\u001b[0;34m(self, T, data, atol, rtol)\u001b[0m\n\u001b[1;32m    212\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\u001b[38;5;241m*\u001b[39mnew_time_steps\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(data\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    213\u001b[0m pde_rhs  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m t,vs: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpde(t,vs) \u001b[38;5;66;03m# make the ODE forward function\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m final_result \u001b[38;5;241m=\u001b[39m \u001b[43modeint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpde_rhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfinal_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrtol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m#breakpoint()\u001b[39;00m\n\u001b[1;32m    216\u001b[0m s_final \u001b[38;5;241m=\u001b[39m final_result[:,:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_ch:,:,:]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mlen\u001b[39m(t),\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_ch,H,W)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchdiffeq/_impl/odeint.py:80\u001b[0m, in \u001b[0;36modeint\u001b[0;34m(func, y0, t, rtol, atol, method, options, event_fn)\u001b[0m\n\u001b[1;32m     77\u001b[0m solver \u001b[38;5;241m=\u001b[39m SOLVERS[method](func\u001b[38;5;241m=\u001b[39mfunc, y0\u001b[38;5;241m=\u001b[39my0, rtol\u001b[38;5;241m=\u001b[39mrtol, atol\u001b[38;5;241m=\u001b[39matol, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m     solution \u001b[38;5;241m=\u001b[39m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintegrate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m     event_t, solution \u001b[38;5;241m=\u001b[39m solver\u001b[38;5;241m.\u001b[39mintegrate_until_event(t[\u001b[38;5;241m0\u001b[39m], event_fn)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchdiffeq/_impl/solvers.py:114\u001b[0m, in \u001b[0;36mFixedGridODESolver.integrate\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m    112\u001b[0m dt \u001b[38;5;241m=\u001b[39m t1 \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\u001b[38;5;241m.\u001b[39mcallback_step(t0, y0, dt)\n\u001b[0;32m--> 114\u001b[0m dy, f0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m y1 \u001b[38;5;241m=\u001b[39m y0 \u001b[38;5;241m+\u001b[39m dy\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m j \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(t) \u001b[38;5;129;01mand\u001b[39;00m t1 \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m t[j]:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchdiffeq/_impl/fixed_grid.py:10\u001b[0m, in \u001b[0;36mEuler._step_func\u001b[0;34m(self, func, t0, dt, t1, y0)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_step_func\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, t0, dt, t1, y0):\n\u001b[0;32m---> 10\u001b[0m     f0 \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperturb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPerturb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNEXT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperturb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mPerturb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNONE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dt \u001b[38;5;241m*\u001b[39m f0, f0\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchdiffeq/_impl/misc.py:197\u001b[0m, in \u001b[0;36m_PerturbFunc.forward\u001b[0;34m(self, t, y, perturb)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Do nothing.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/climode-reproducibility/climode/ClimODE/model_function.py:213\u001b[0m, in \u001b[0;36mClimate_encoder_free_uncertain.forward.<locals>.<lambda>\u001b[0;34m(t, vs)\u001b[0m\n\u001b[1;32m    211\u001b[0m new_time_steps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(init_time,final_time,steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(steps_val)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(data\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    212\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\u001b[38;5;241m*\u001b[39mnew_time_steps\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(data\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m--> 213\u001b[0m pde_rhs  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m t,vs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpde\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# make the ODE forward function\u001b[39;00m\n\u001b[1;32m    214\u001b[0m final_result \u001b[38;5;241m=\u001b[39m odeint(pde_rhs,final_data,t,method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod,atol\u001b[38;5;241m=\u001b[39matol,rtol\u001b[38;5;241m=\u001b[39mrtol)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m#breakpoint()\u001b[39;00m\n",
      "File \u001b[0;32m~/climode-reproducibility/climode/ClimODE/model_function.py:140\u001b[0m, in \u001b[0;36mClimate_encoder_free_uncertain.pde\u001b[0;34m(self, t, vs)\u001b[0m\n\u001b[1;32m    137\u001b[0m v_y \u001b[38;5;241m=\u001b[39m v[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_ch:,:,:]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_ch,vs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m],vs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m])\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    139\u001b[0m adv1 \u001b[38;5;241m=\u001b[39m v_x\u001b[38;5;241m*\u001b[39mds_grad_x \u001b[38;5;241m+\u001b[39m v_y\u001b[38;5;241m*\u001b[39mds_grad_y\n\u001b[0;32m--> 140\u001b[0m adv2 \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m*\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mgradient(v_y,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)[\u001b[38;5;241m0\u001b[39m] )\n\u001b[1;32m    143\u001b[0m ds \u001b[38;5;241m=\u001b[39m adv1 \u001b[38;5;241m+\u001b[39m adv2\n\u001b[1;32m    145\u001b[0m dvs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([dv,ds],\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utils import nll\n",
    "\n",
    "for epoch in range(args.niters):\n",
    "    print(f\"##### Epoch {epoch} of {args.niters} #####\")\n",
    "    total_train_loss = 0\n",
    "    val_loss = 0\n",
    "    test_loss = 0\n",
    "    # RMSD = []\n",
    "    # breakpoint()\n",
    "    if epoch == 0:\n",
    "        var_coeff = 0.001\n",
    "    else:\n",
    "        var_coeff = 2 * scheduler.get_last_lr()[0]\n",
    "\n",
    "    _total = min(len(time_loader), len(Train_loader))\n",
    "    pbar = tqdm(\n",
    "        enumerate(zip(time_loader, Train_loader)),\n",
    "        total=_total,\n",
    "        colour=\"green\",\n",
    "        desc=\"train\",\n",
    "    )\n",
    "    for entry, (time_steps, batch) in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        data = (\n",
    "            batch[0]\n",
    "            .to(device)\n",
    "            .view(num_years, 1, len(paths_to_data) * (args.scale + 1), H, W)\n",
    "        )\n",
    "        past_sample = (\n",
    "            vel_train[entry]\n",
    "            .view(num_years, 2 * len(paths_to_data) * (args.scale + 1), H, W)\n",
    "            .to(device)\n",
    "        )\n",
    "        model.update_param(\n",
    "            [\n",
    "                past_sample,\n",
    "                const_channels_info.to(device),\n",
    "                lat_map.to(device),\n",
    "                lon_map.to(device),\n",
    "            ]\n",
    "        )\n",
    "        t = time_steps.float().to(device).flatten()\n",
    "        mean, std, _ = model(t, data)\n",
    "        loss = nll(mean, std, batch.float().to(device), lat, var_coeff)\n",
    "        l2_lambda = 0.001\n",
    "        l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "        loss = loss + l2_lambda * l2_norm\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(\"Loss for batch is \",loss.item())\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "        if torch.isnan(loss):\n",
    "            print(\"Quitting due to Nan loss\")\n",
    "            quit()\n",
    "        total_train_loss = total_train_loss + loss.item()\n",
    "\n",
    "    lr_val = scheduler.get_last_lr()[0]\n",
    "    scheduler.step()\n",
    "    print(\"|Iter \", epoch, \" | Total Train Loss \", total_train_loss, \"|\")\n",
    "    optimizer.zero_grad(set_to_none=True)  # Clear memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(\n",
    "            enumerate(zip(time_loader, Val_loader)),\n",
    "            total=min(len(time_loader), len(Val_loader)),\n",
    "            colour=\"blue\",\n",
    "            desc=\"test\",\n",
    "        )\n",
    "        for entry, (time_steps, batch) in pbar:\n",
    "            data = (\n",
    "                batch[0]\n",
    "                .to(device)\n",
    "                .view(1, 1, len(paths_to_data) * (args.scale + 1), H, W)\n",
    "            )\n",
    "            past_sample = (\n",
    "                vel_val[entry]\n",
    "                .view(1, 2 * len(paths_to_data) * (args.scale + 1), H, W)\n",
    "                .to(device)\n",
    "            )\n",
    "            model.update_param(\n",
    "                [\n",
    "                    past_sample,\n",
    "                    const_channels_info.to(device),\n",
    "                    lat_map.to(device),\n",
    "                    lon_map.to(device),\n",
    "                ]\n",
    "            )\n",
    "            t = time_steps.float().to(device).flatten()\n",
    "            mean, std, _ = model(t, data)\n",
    "            loss = nll(mean, std, batch.float().to(device), lat, var_coeff)\n",
    "            if torch.isnan(loss):\n",
    "                print(\"Quitting due to Nan loss\")\n",
    "                quit()\n",
    "            pbar.set_postfix({\"val_lss\": loss.item()})\n",
    "            val_loss = val_loss + loss.item()\n",
    "\n",
    "    print(\"|Iter \", epoch, \" | Total Val Loss \", val_loss, \"|\")\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(\n",
    "            model,\n",
    "            str(cwd)\n",
    "            + \"/Models/\"\n",
    "            + \"ClimODE_global_\"\n",
    "            + args.solver\n",
    "            + \"_\"\n",
    "            + str(args.spectral)\n",
    "            + \"_model_\"\n",
    "            + str(epoch)\n",
    "            + \".pt\",\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
